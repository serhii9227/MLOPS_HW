{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ebe5600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 14:28:02.729466: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import mlflow\n",
    "from mlflow.pyfunc import load_model\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.exceptions import RestException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31bb5ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"mlops93\"\n",
    "s3_key = \"processed_data/combined_data.csv\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=s3_key)\n",
    "data = pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')))\n",
    "        \n",
    "cols = list(data.columns)\n",
    "text_col = [col for col in cols if col != 'label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b1970fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(data['label'])\n",
    "messages = list(data[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3184713",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12698fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_entity(raw_text):\n",
    "            entity_regex = r\"&[^\\s;]+;\"\n",
    "            text = re.sub(entity_regex, \"\", raw_text)\n",
    "            return text\n",
    "        \n",
    "def change_user(raw_text):\n",
    "    regex = r\"@([^ ]+)\"\n",
    "    text = re.sub(regex, \"user\", raw_text)\n",
    "    return text\n",
    "\n",
    "def remove_url(raw_text):\n",
    "    url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»\"\"'']))\"\n",
    "    text = re.sub(url_regex, '', raw_text)\n",
    "    return text\n",
    "\n",
    "def remove_noise_symbols(raw_text):\n",
    "    text = raw_text.replace('\"', '')\n",
    "    text = text.replace(\"'\", '')\n",
    "    text = text.replace(\"!\", '')\n",
    "    text = text.replace(\"`\", '')\n",
    "    text = text.replace(\"..\", '')\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(raw_text):\n",
    "    try:\n",
    "        tokenize = nltk.word_tokenize(raw_text)\n",
    "        text = [word for word in tokenize if not word.lower() in stop_words]\n",
    "        text = \" \".join(text)\n",
    "        return text\n",
    "    except:\n",
    "        return raw_text\n",
    "\n",
    "def preprocess(datas):\n",
    "    clean = []\n",
    "    clean = [change_user(text) for text in datas]\n",
    "    clean = [remove_entity(text) for text in clean]\n",
    "    clean = [remove_url(text) for text in clean]\n",
    "    clean = [remove_noise_symbols(text) for text in clean]\n",
    "    clean = [remove_stopwords(text) for text in clean]\n",
    "    return clean\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8c1d041",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_messages = preprocess(messages)\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_messages, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "530005bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_KEY = \"tokenizer.pkl\"\n",
    "response = s3.get_object(Bucket=bucket_name, Key=TOKENIZER_KEY)\n",
    "tokenizer = pickle.loads(response['Body'].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e69569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed055abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = max(len(seq) for seq in X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=max_length)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "336c72ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(y_test)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d2ff678",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8bd46dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/so/Documents/ucu/MLOPS/HW3_MLFlow/venv/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, output_dim, input_length=max_length),\n",
    "    LSTM(64, dropout=0.3, recurrent_dropout=0.3),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43c52f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3f8a740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m616s\u001b[0m 1s/step - accuracy: 0.5905 - loss: 0.6523 - precision: 0.5905 - recall: 0.6178 - val_accuracy: 0.7174 - val_loss: 0.5320 - val_precision: 0.7480 - val_recall: 0.6502\n",
      "Epoch 2/5\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m610s\u001b[0m 1s/step - accuracy: 0.7626 - loss: 0.4744 - precision: 0.7653 - recall: 0.7584 - val_accuracy: 0.7275 - val_loss: 0.5111 - val_precision: 0.7097 - val_recall: 0.7639\n",
      "Epoch 3/5\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1833s\u001b[0m 4s/step - accuracy: 0.8229 - loss: 0.3699 - precision: 0.8177 - recall: 0.8325 - val_accuracy: 0.7394 - val_loss: 0.5254 - val_precision: 0.7486 - val_recall: 0.7159\n",
      "Epoch 4/5\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3813s\u001b[0m 7s/step - accuracy: 0.8648 - loss: 0.2984 - precision: 0.8602 - recall: 0.8722 - val_accuracy: 0.7436 - val_loss: 0.6130 - val_precision: 0.7434 - val_recall: 0.7392\n",
      "Epoch 5/5\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m592s\u001b[0m 1s/step - accuracy: 0.8882 - loss: 0.2448 - precision: 0.8819 - recall: 0.8965 - val_accuracy: 0.7406 - val_loss: 0.6721 - val_precision: 0.7304 - val_recall: 0.7576\n"
     ]
    }
   ],
   "source": [
    "model_history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=5,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80619110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 120ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred >= 0.5).astype(int)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de4a005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"HateSpeechLSTM\"\n",
    "EXPERIMENT_NAME = \"Detect hate speech\"\n",
    "BUCKET_NAME = \"mlops93\"\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55327264",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "if experiment is None:\n",
    "    experiment_id = client.create_experiment(EXPERIMENT_NAME)\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "\n",
    "# check existing model\n",
    "try:\n",
    "    latest_versions = client.get_latest_versions(MODEL_NAME, stages=[\"Production\"])\n",
    "    first_model = len(latest_versions) == 0\n",
    "except RestException:\n",
    "    latest_versions = []\n",
    "    first_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8dec2a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ModelVersion: aliases=[], creation_timestamp=1753184599538, current_stage='Production', description='', last_updated_timestamp=1753184599580, name='HateSpeechLSTM', run_id='8947c30d0711404c86efaa0b21ffcc88', run_link='', source='runs:/8947c30d0711404c86efaa0b21ffcc88/model', status='READY', status_message='', tags={}, user_id='', version='1'>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df5b97e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:04<00:00,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Помилка при завантаженні попередньої моделі: No such file or directory: '/var/folders/7m/2q7981kj2f5dhx3p04qffkgh0000gn/T/tmp6bm0c074/MLmodel'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare with previous model\n",
    "if not first_model:\n",
    "    try:\n",
    "        previous_model_uri = f\"models:/{MODEL_NAME}/{latest_versions[0].version}\"\n",
    "        previous_model = mlflow.pyfunc.load_model(previous_model_uri)\n",
    "        y_pred_prev = previous_model.predict(X_test)\n",
    "        y_pred_prev = (y_pred_prev >= 0.5).astype(int)\n",
    "        f1_previous = f1_score(y_test, y_pred_prev, zero_division=0)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Помилка при завантаженні попередньої моделі: {e}\")\n",
    "        f1_previous = -1\n",
    "else:\n",
    "    f1_previous = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea80b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:08<00:00,  8.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 135ms/step\n"
     ]
    }
   ],
   "source": [
    "# Compare with previous model\n",
    "\n",
    "if not first_model:\n",
    "    try:\n",
    "        model_info = client.get_model_version(name=MODEL_NAME, version=latest_versions[0].version)\n",
    "        artifact_uri = model_info.source  # типу: runs:/<run_id>/model\n",
    "\n",
    "        local_path = mlflow.artifacts.download_artifacts(artifact_uri)\n",
    "\n",
    "        model_path = os.path.join(local_path, \"model.keras\")\n",
    "        previous_model = load_model(model_path)\n",
    "\n",
    "        y_pred_prev = previous_model.predict(X_test)\n",
    "        y_pred_prev = (y_pred_prev >= 0.5).astype(int)\n",
    "        f1_previous = f1_score(y_test, y_pred_prev, zero_division=0)\n",
    "    except Exception as e:\n",
    "        f1_previous = -1\n",
    "else:\n",
    "    f1_previous = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c12c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/22 17:06:32 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: HateSpeechLSTM, version 2\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:07<00:00,  7.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📤 Завантажено до s3://mlops93/artifacts/v2/model/model.keras\n"
     ]
    }
   ],
   "source": [
    "# Register model\n",
    "if first_model or f1 > f1_previous:\n",
    "    with mlflow.start_run(experiment_id=experiment_id) as run:\n",
    "        mlflow.log_metric(\"f1\", f1)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "        model_path = \"model.keras\"\n",
    "        model.save(model_path)\n",
    "\n",
    "        mlflow.log_artifact(model_path, artifact_path=\"model\")\n",
    "        \n",
    "        model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "        \n",
    "        # create model in register\n",
    "        try:\n",
    "            client.create_registered_model(MODEL_NAME)\n",
    "        except RestException:\n",
    "            pass\n",
    "        \n",
    "        # Register new version\n",
    "        model_version = client.create_model_version(\n",
    "            name=MODEL_NAME,\n",
    "            source=model_uri,\n",
    "            run_id=run.info.run_id\n",
    "        )\n",
    "        \n",
    "        # Archive previous verion\n",
    "        for v in client.get_latest_versions(MODEL_NAME, stages=[\"Production\"]):\n",
    "            if v.version != model_version.version:\n",
    "                client.transition_model_version_stage(\n",
    "                    name=MODEL_NAME,\n",
    "                    version=v.version,\n",
    "                    stage=\"Archived\"\n",
    "                )\n",
    "        \n",
    "        # Promote new version\n",
    "        client.transition_model_version_stage(\n",
    "            name=MODEL_NAME,\n",
    "            version=model_version.version,\n",
    "            stage=\"Production\"\n",
    "        )\n",
    "\n",
    " # Завантаження артефактів\n",
    "        local_path = os.path.join(\"/tmp\", \"artifacts\", f\"v{model_version.version}\")\n",
    "        os.makedirs(local_path, exist_ok=True)\n",
    "        mlflow.artifacts.download_artifacts(artifact_uri=model_uri, dst_path=local_path)\n",
    "        \n",
    "        # Завантаження до S3\n",
    "        def upload_dir_to_s3(local_dir, bucket, s3_prefix):\n",
    "            for root, dirs, files in os.walk(local_dir):\n",
    "                for file in files:\n",
    "                    full_path = os.path.join(root, file)\n",
    "                    relative_path = os.path.relpath(full_path, local_dir)\n",
    "                    s3_key = os.path.join(s3_prefix, relative_path).replace(\"\\\\\", \"/\")\n",
    "                    s3.upload_file(full_path, bucket, s3_key)\n",
    "        \n",
    "        upload_dir_to_s3(local_path, BUCKET_NAME, f\"artifacts/v{model_version.version}\")\n",
    "else:\n",
    "    print(\"Model is not better\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
